{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MILK10k Skin Lesion Classification (Standalone)\n",
                "\n",
                "This notebook trains the **Tone-Aware Multi-Scale Vision Transformer (TAM-ViT)** on the MILK10k dataset.\n",
                "It is self-contained and includes all necessary model and data classes.\n",
                "\n",
                "## ðŸš€ Setup\n",
                "\n",
                "1.  **Enable GPU**: Go to `Runtime` -> `Change runtime type` -> `T4 GPU` (or better).\n",
                "2.  **Upload Data**: You need to upload your `milk10k` dataset folder to your Drive or directly here.\n",
                "    - Expected structure:\n",
                "        ```\n",
                "        /content/data/milk10k/\n",
                "        â”œâ”€â”€ train/\n",
                "        â”‚   â”œâ”€â”€ lesion1_clin.jpg\n",
                "        â”‚   â”œâ”€â”€ lesion1_derm.jpg\n",
                "        â”‚   â””â”€â”€ ...\n",
                "        â”œâ”€â”€ train.csv\n",
                "        â””â”€â”€ val.csv\n",
                "        ```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install torch torchvision timm albumentations pandas numpy omegaconf pytorch-lightning wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Imports\n",
                "import warnings\n",
                "# Suppress non-critical warnings for cleaner output\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
                "\n",
                "import os\n",
                "import math\n",
                "import sys\n",
                "from pathlib import Path\n",
                "from typing import Optional, Dict, Any, List, Tuple, Callable\n",
                "from datetime import datetime\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
                "from torch.optim import AdamW\n",
                "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR, SequentialLR\n",
                "\n",
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from einops import rearrange, repeat\n",
                "# Updated import for newer timm versions to suppress warnings\n",
                "try:\n",
                "    from timm.layers import DropPath, trunc_normal_\n",
                "except ImportError:\n",
                "    from timm.models.layers import DropPath, trunc_normal_\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, RichProgressBar\n",
                "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
                "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# Transforms\n",
                "# =============================================================================\n",
                "def get_train_transforms(img_size: int = 224) -> A.Compose:\n",
                "    return A.Compose([\n",
                "        A.RandomResizedCrop(img_size, img_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
                "        A.HorizontalFlip(p=0.5),\n",
                "        A.VerticalFlip(p=0.5),\n",
                "        A.RandomRotate90(p=0.5),\n",
                "        A.OneOf([\n",
                "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
                "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20),\n",
                "        ], p=0.5),\n",
                "        A.OneOf([\n",
                "            A.GaussNoise(var_limit=(10, 50)),\n",
                "            A.GaussianBlur(blur_limit=(3, 5)),\n",
                "        ], p=0.3),\n",
                "        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.2),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2(),\n",
                "    ])\n",
                "\n",
                "def get_val_transforms(img_size: int = 224) -> A.Compose:\n",
                "    return A.Compose([\n",
                "        A.Resize(int(img_size * 1.14), int(img_size * 1.14)),\n",
                "        A.CenterCrop(img_size, img_size),\n",
                "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ToTensorV2(),\n",
                "    ])\n",
                "\n",
                "# =============================================================================\n",
                "# MILK10k Dataset\n",
                "# =============================================================================\n",
                "class MILK10kDataset(Dataset):\n",
                "    CLASS_NAMES = [\n",
                "        'AKIEC', 'BCC', 'BEN_OTH', 'BKL', 'DF', 'INF', \n",
                "        'MAL_OTH', 'MEL', 'NV', 'SCCKA', 'VASC'\n",
                "    ]\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        root_dir: str,\n",
                "        csv_file: str,\n",
                "        transform: Optional[A.Compose] = None,\n",
                "        phase: str = 'train',\n",
                "    ):\n",
                "        self.root_dir = Path(root_dir)\n",
                "        self.transform = transform\n",
                "        self.phase = phase\n",
                "        self.df = pd.read_csv(csv_file)\n",
                "        self.label_map = {name: idx for idx, name in enumerate(self.CLASS_NAMES)}\n",
                "        \n",
                "    def __len__(self) -> int:\n",
                "        return len(self.df)\n",
                "    \n",
                "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
                "        row = self.df.iloc[idx]\n",
                "        lesion_id = row['lesion_id']\n",
                "        \n",
                "        clinical_path = self.root_dir / row.get('clinical_image_name', f\"{lesion_id}_clin.jpg\") \n",
                "        dermoscopic_path = self.root_dir / row.get('dermoscopic_image_name', f\"{lesion_id}_derm.jpg\")\n",
                "        \n",
                "        if not clinical_path.exists(): clinical_path = clinical_path.with_suffix('.png')\n",
                "        if not dermoscopic_path.exists(): dermoscopic_path = dermoscopic_path.with_suffix('.png')\n",
                "            \n",
                "        try:\n",
                "            img_clin = np.array(Image.open(clinical_path).convert('RGB'))\n",
                "            img_derm = np.array(Image.open(dermoscopic_path).convert('RGB'))\n",
                "        except FileNotFoundError:\n",
                "            img_clin = np.zeros((224, 224, 3), dtype=np.uint8)\n",
                "            img_derm = np.zeros((224, 224, 3), dtype=np.uint8)\n",
                "            print(f\"Warning: Missing images for {lesion_id}\")\n",
                "\n",
                "        if self.transform:\n",
                "            img_clin = self.transform(image=img_clin)['image']\n",
                "            img_derm = self.transform(image=img_derm)['image']\n",
                "\n",
                "        # Stack images: (6, H, W)\n",
                "        image_stacked = torch.cat([img_clin, img_derm], dim=0)\n",
                "        \n",
                "        label = -1\n",
                "        if self.phase != 'test':\n",
                "            for col in ['diagnosis', 'pathology', 'dx']:\n",
                "                if col in row:\n",
                "                    diagnosis = row[col]\n",
                "                    if diagnosis in self.label_map:\n",
                "                        label = self.label_map[diagnosis]\n",
                "                        break\n",
                "        \n",
                "        meta = {\n",
                "            'age': float(row.get('age', -1)),\n",
                "            'sex': 1 if row.get('sex') == 'male' else 0,\n",
                "            'fitzpatrick': int(row.get('skin_tone', -1)),\n",
                "            'anatom_site': row.get('anatom_site', 'unknown')\n",
                "        }\n",
                "\n",
                "        return {\n",
                "            'image': image_stacked,\n",
                "            'label': torch.tensor(label, dtype=torch.long),\n",
                "            'fitzpatrick': torch.tensor(meta['fitzpatrick'], dtype=torch.long),\n",
                "            'lesion_id': lesion_id,\n",
                "        }\n",
                "\n",
                "def create_weighted_sampler(dataset: Dataset) -> WeightedRandomSampler:\n",
                "    labels = []\n",
                "    for i in range(len(dataset)):\n",
                "        item = dataset[i]\n",
                "        labels.append(item['label'].item())\n",
                "    labels = np.array(labels)\n",
                "    class_counts = np.bincount(labels)\n",
                "    class_weights = 1.0 / (class_counts + 1e-8)\n",
                "    sample_weights = class_weights[labels]\n",
                "    return WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
                "\n",
                "def create_dataloaders(train_dataset, val_dataset, batch_size=32, num_workers=4):\n",
                "    train_sampler = create_weighted_sampler(train_dataset)\n",
                "    return {\n",
                "        'train': DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers),\n",
                "        'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Classes (TAM-ViT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# TAM-ViT Modules\n",
                "# =============================================================================\n",
                "class PatchEmbed(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
                "        super().__init__()\n",
                "        self.img_size = img_size\n",
                "        self.patch_size = patch_size\n",
                "        self.num_patches = (img_size // patch_size) ** 2\n",
                "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
                "        self.norm = nn.LayerNorm(embed_dim)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.proj(x)\n",
                "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
                "        x = self.norm(x)\n",
                "        return x\n",
                "\n",
                "class SkinToneEstimator(nn.Module):\n",
                "    def __init__(self, num_tones=6, hidden_dim=256):\n",
                "        super().__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
                "            nn.AdaptiveAvgPool2d(1),\n",
                "        )\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(128, hidden_dim), nn.ReLU(inplace=True), nn.Dropout(0.2), nn.Linear(hidden_dim, num_tones),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        features = self.features(x).flatten(1)\n",
                "        tone_logits = self.classifier(features)\n",
                "        tone_probs = F.softmax(tone_logits, dim=-1)\n",
                "        return tone_logits, tone_probs\n",
                "\n",
                "class ToneAdaptiveLayerNorm(nn.Module):\n",
                "    def __init__(self, dim, tone_dim=768):\n",
                "        super().__init__()\n",
                "        self.norm = nn.LayerNorm(dim)\n",
                "        self.gamma_proj = nn.Sequential(nn.Linear(tone_dim, dim // 2), nn.ReLU(inplace=True), nn.Linear(dim // 2, dim))\n",
                "        self.beta_proj = nn.Sequential(nn.Linear(tone_dim, dim // 2), nn.ReLU(inplace=True), nn.Linear(dim // 2, dim))\n",
                "        nn.init.zeros_(self.gamma_proj[-1].weight)\n",
                "        nn.init.zeros_(self.gamma_proj[-1].bias)\n",
                "        nn.init.zeros_(self.beta_proj[-1].weight)\n",
                "        nn.init.zeros_(self.beta_proj[-1].bias)\n",
                "    \n",
                "    def forward(self, x, tone_embed):\n",
                "        normalized = self.norm(x)\n",
                "        gamma = 1 + self.gamma_proj(tone_embed).unsqueeze(1)\n",
                "        beta = self.beta_proj(tone_embed).unsqueeze(1)\n",
                "        return gamma * normalized + beta\n",
                "\n",
                "class MultiHeadSelfAttention(nn.Module):\n",
                "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0.0, proj_drop=0.0):\n",
                "        super().__init__()\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = dim // num_heads\n",
                "        self.scale = self.head_dim ** -0.5\n",
                "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
                "        self.attn_drop = nn.Dropout(attn_drop)\n",
                "        self.proj = nn.Linear(dim, dim)\n",
                "        self.proj_drop = nn.Dropout(proj_drop)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        B, N, C = x.shape\n",
                "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
                "        q, k, v = qkv.unbind(0)\n",
                "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
                "        attn = attn.softmax(dim=-1)\n",
                "        attn = self.attn_drop(attn)\n",
                "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
                "        x = self.proj(x)\n",
                "        x = self.proj_drop(x)\n",
                "        return x, attn\n",
                "\n",
                "class ToneModulatedMLP(nn.Module):\n",
                "    def __init__(self, in_features, hidden_features=None, out_features=None, tone_dim=768, drop=0.0):\n",
                "        super().__init__()\n",
                "        out_features = out_features or in_features\n",
                "        hidden_features = hidden_features or in_features * 4\n",
                "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
                "        self.act = nn.GELU()\n",
                "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
                "        self.drop = nn.Dropout(drop)\n",
                "        self.gate = nn.Sequential(\n",
                "            nn.Linear(tone_dim, hidden_features // 2), nn.ReLU(inplace=True),\n",
                "            nn.Linear(hidden_features // 2, out_features), nn.Sigmoid(),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x, tone_embed):\n",
                "        x = self.fc1(x)\n",
                "        x = self.act(x)\n",
                "        x = self.drop(x)\n",
                "        x = self.fc2(x)\n",
                "        gate = self.gate(tone_embed).unsqueeze(1)\n",
                "        x = x * gate\n",
                "        x = self.drop(x)\n",
                "        return x\n",
                "\n",
                "class ToneConditionedBlock(nn.Module):\n",
                "    def __init__(self, dim, num_heads, tone_dim=768, mlp_ratio=4.0, qkv_bias=True, drop=0.0, attn_drop=0.0, drop_path=0.0):\n",
                "        super().__init__()\n",
                "        self.norm1 = ToneAdaptiveLayerNorm(dim, tone_dim)\n",
                "        self.attn = MultiHeadSelfAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
                "        self.norm2 = ToneAdaptiveLayerNorm(dim, tone_dim)\n",
                "        self.mlp = ToneModulatedMLP(in_features=dim, hidden_features=int(dim * mlp_ratio), tone_dim=tone_dim, drop=drop)\n",
                "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
                "    \n",
                "    def forward(self, x, tone_embed):\n",
                "        attn_out, attn_weights = self.attn(self.norm1(x, tone_embed))\n",
                "        x = x + self.drop_path(attn_out)\n",
                "        x = x + self.drop_path(self.mlp(self.norm2(x, tone_embed), tone_embed))\n",
                "        return x, attn_weights\n",
                "\n",
                "class MultiScalePatchMerger(nn.Module):\n",
                "    def __init__(self, embed_dim, num_heads=8):\n",
                "        super().__init__()\n",
                "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        self.proj = nn.Linear(embed_dim * 2, embed_dim)\n",
                "    \n",
                "    def forward(self, coarse, fine):\n",
                "        attended, _ = self.cross_attn(self.norm1(coarse), self.norm2(fine), fine)\n",
                "        merged = torch.cat([coarse, attended], dim=-1)\n",
                "        merged = self.proj(merged)\n",
                "        return merged\n",
                "\n",
                "class UncertaintyHead(nn.Module):\n",
                "    def __init__(self, embed_dim, num_classes, hidden_dim=256):\n",
                "        super().__init__()\n",
                "        self.variance_head = nn.Sequential(\n",
                "            nn.Linear(embed_dim, hidden_dim), nn.ReLU(inplace=True), nn.Dropout(0.1),\n",
                "            nn.Linear(hidden_dim, num_classes), nn.Softplus()\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.variance_head(x) + 1e-6\n",
                "\n",
                "class TAMViT(nn.Module):\n",
                "    def __init__(self, img_size=224, patch_sizes=[16, 8], in_chans=3, num_classes=9, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, num_tones=6):\n",
                "        super().__init__()\n",
                "        self.num_classes = num_classes\n",
                "        self.embed_dim = embed_dim\n",
                "        self.tone_estimator = SkinToneEstimator(num_tones=num_tones)\n",
                "        self.tone_embed = nn.Sequential(nn.Linear(num_tones, embed_dim // 2), nn.ReLU(inplace=True), nn.Linear(embed_dim // 2, embed_dim))\n",
                "        self.patch_embeds = nn.ModuleList([PatchEmbed(img_size, ps, in_chans, embed_dim) for ps in patch_sizes])\n",
                "        self.num_patches = self.patch_embeds[0].num_patches\n",
                "        self.patch_merger = MultiScalePatchMerger(embed_dim, num_heads)\n",
                "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
                "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
                "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
                "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
                "        self.blocks = nn.ModuleList([\n",
                "            ToneConditionedBlock(dim=embed_dim, num_heads=num_heads, tone_dim=embed_dim, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i])\n",
                "            for i in range(depth)\n",
                "        ])\n",
                "        self.norm = nn.LayerNorm(embed_dim)\n",
                "        self.cls_head = nn.Sequential(nn.Linear(embed_dim, 256), nn.GELU(), nn.Dropout(drop_rate), nn.Linear(256, num_classes))\n",
                "        self.uncertainty_head = UncertaintyHead(embed_dim, num_classes)\n",
                "        self._init_weights()\n",
                "    \n",
                "    def _init_weights(self):\n",
                "        trunc_normal_(self.pos_embed, std=0.02)\n",
                "        trunc_normal_(self.cls_token, std=0.02)\n",
                "        self.apply(self._init_weights_module)\n",
                "    \n",
                "    def _init_weights_module(self, m):\n",
                "        if isinstance(m, nn.Linear):\n",
                "            trunc_normal_(m.weight, std=0.02)\n",
                "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
                "        elif isinstance(m, nn.LayerNorm):\n",
                "            nn.init.zeros_(m.bias)\n",
                "            nn.init.ones_(m.weight)\n",
                "    \n",
                "    def forward(self, x, return_uncertainty=True, return_attention=False):\n",
                "        B = x.shape[0]\n",
                "        # Handle 6-channel input for tone estimation\n",
                "        img_for_tone = x[:, 3:, :, :] if x.shape[1] == 6 else x\n",
                "        tone_logits, tone_probs = self.tone_estimator(img_for_tone)\n",
                "        tone_embedding = self.tone_embed(tone_probs)\n",
                "        \n",
                "        coarse_patches = self.patch_embeds[0](x)\n",
                "        if len(self.patch_embeds) > 1:\n",
                "            fine_patches = self.patch_embeds[1](x)\n",
                "            patches = self.patch_merger(coarse_patches, fine_patches)\n",
                "        else:\n",
                "            patches = coarse_patches\n",
                "        \n",
                "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
                "        x = torch.cat([cls_tokens, patches], dim=1)\n",
                "        x = x + self.pos_embed\n",
                "        x = self.pos_drop(x)\n",
                "        \n",
                "        attentions = []\n",
                "        for block in self.blocks:\n",
                "            x, attn = block(x, tone_embedding)\n",
                "            if return_attention: attentions.append(attn)\n",
                "        \n",
                "        x = self.norm(x)\n",
                "        features = x[:, 0]\n",
                "        logits = self.cls_head(features)\n",
                "        probs = F.softmax(logits, dim=-1)\n",
                "        \n",
                "        result = {'logits': logits, 'probs': probs, 'tone_probs': tone_probs}\n",
                "        if return_uncertainty:\n",
                "            variance = self.uncertainty_head(features)\n",
                "            result['variance'] = variance\n",
                "            result['uncertainty'] = variance.sum(dim=-1)\n",
                "        if return_attention:\n",
                "            result['attention'] = torch.stack(attentions, dim=1)\n",
                "        return result\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def predict_with_mc_dropout(self, x, n_samples=30):\n",
                "        was_training = self.training\n",
                "        self.train()\n",
                "        predictions = []\n",
                "        for _ in range(n_samples):\n",
                "            out = self.forward(x, return_uncertainty=True)\n",
                "            predictions.append(out['probs'])\n",
                "        preds = torch.stack(predictions)\n",
                "        mean_pred = preds.mean(dim=0)\n",
                "        epistemic = preds.var(dim=0).sum(dim=-1)\n",
                "        aleatoric = -torch.sum(mean_pred * torch.log(mean_pred + 1e-8), dim=-1)\n",
                "        if not was_training: self.eval()\n",
                "        return {'mean_probs': mean_pred, 'epistemic_uncertainty': epistemic, 'aleatoric_uncertainty': aleatoric}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Loss Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
                "        super().__init__()\n",
                "        self.gamma = gamma\n",
                "        self.alpha = alpha\n",
                "        self.reduction = reduction\n",
                "    \n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
                "        if self.alpha is not None:\n",
                "            alpha_t = self.alpha.to(inputs.device)[targets]\n",
                "            focal_loss = alpha_t * focal_loss\n",
                "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
                "\n",
                "class UncertaintyAwareLoss(nn.Module):\n",
                "    def __init__(self, reduction='mean'):\n",
                "        super().__init__()\n",
                "        self.reduction = reduction\n",
                "    \n",
                "    def forward(self, logits, targets, variance):\n",
                "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
                "        target_variance = variance[torch.arange(len(targets)), targets]\n",
                "        nll = 0.5 * (torch.log(target_variance + 1e-8) + ce_loss / (target_variance + 1e-8))\n",
                "        return nll.mean() if self.reduction == 'mean' else nll.sum()\n",
                "\n",
                "class DermEquityLoss(nn.Module):\n",
                "    def __init__(self, num_classes=9, gamma=2.0, lambda_unc=0.1, lambda_fair=0.5, class_weights=None):\n",
                "        super().__init__()\n",
                "        self.focal_loss = FocalLoss(gamma=gamma, alpha=class_weights)\n",
                "        self.uncertainty_loss = UncertaintyAwareLoss()\n",
                "        self.lambda_unc = lambda_unc\n",
                "        self.lambda_fair = lambda_fair\n",
                "    \n",
                "    def forward(self, outputs, targets):\n",
                "        logits = outputs['logits']\n",
                "        focal = self.focal_loss(logits, targets)\n",
                "        total = focal\n",
                "        loss_dict = {'focal': focal.item()}\n",
                "        if 'variance' in outputs:\n",
                "            unc = self.uncertainty_loss(logits, targets, outputs['variance'])\n",
                "            total += self.lambda_unc * unc\n",
                "            loss_dict['uncertainty'] = unc.item()\n",
                "        return total, loss_dict\n",
                "\n",
                "def compute_class_weights(labels, num_classes):\n",
                "    counts = torch.bincount(labels, minlength=num_classes).float()\n",
                "    weights = 1.0 / (counts + 1e-8)\n",
                "    weights = weights / weights.sum() * num_classes\n",
                "    return weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DermEquityModule(pl.LightningModule):\n",
                "    def __init__(self, model_config, train_config, class_weights=None):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.model = TAMViT(\n",
                "            img_size=model_config.get('img_size', 224),\n",
                "            patch_sizes=model_config.get('patch_sizes', [16, 8]),\n",
                "            num_classes=model_config.get('num_classes', 9),\n",
                "            embed_dim=model_config.get('embed_dim', 768),\n",
                "            depth=model_config.get('depth', 12),\n",
                "            num_heads=model_config.get('num_heads', 12),\n",
                "            mlp_ratio=model_config.get('mlp_ratio', 4.0),\n",
                "            drop_rate=model_config.get('dropout', 0.1),\n",
                "            drop_path_rate=model_config.get('drop_path', 0.1),\n",
                "            in_chans=model_config.get('in_chans', 3)  # Use 6 for MILK10k\n",
                "        )\n",
                "        self.criterion = DermEquityLoss(\n",
                "            num_classes=model_config.get('num_classes', 9),\n",
                "            gamma=train_config.get('focal_gamma', 2.0),\n",
                "            lambda_unc=train_config.get('lambda_unc', 0.1),\n",
                "            lambda_fair=train_config.get('lambda_fair', 0.5),\n",
                "            class_weights=class_weights,\n",
                "        )\n",
                "        self.train_config = train_config\n",
                "        self.model_config = model_config\n",
                "        self.val_preds = []; self.val_labels = []\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.model(x, return_uncertainty=True)\n",
                "\n",
                "    def training_step(self, batch, batch_idx):\n",
                "        images, labels = batch['image'], batch['label']\n",
                "        outputs = self.model(images, return_uncertainty=True)\n",
                "        loss, loss_dict = self.criterion(outputs, labels)\n",
                "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
                "        return loss\n",
                "\n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        images, labels = batch['image'], batch['label']\n",
                "        outputs = self.model(images, return_uncertainty=True)\n",
                "        loss, loss_dict = self.criterion(outputs, labels)\n",
                "        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
                "        self.val_preds.append(outputs['probs'].detach().cpu())\n",
                "        self.val_labels.append(labels.cpu())\n",
                "\n",
                "    def on_validation_epoch_end(self):\n",
                "        all_probs = torch.cat(self.val_preds, dim=0).numpy()\n",
                "        all_labels = torch.cat(self.val_labels, dim=0).numpy()\n",
                "        all_preds = np.argmax(all_probs, axis=1)\n",
                "        try:\n",
                "            auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')\n",
                "        except ValueError: auc = 0.0\n",
                "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
                "        self.log('val/auc_roc', auc, prog_bar=True)\n",
                "        self.log('val/f1_macro', f1, prog_bar=True)\n",
                "        self.val_preds.clear(); self.val_labels.clear()\n",
                "\n",
                "    def configure_optimizers(self):\n",
                "        optimizer = AdamW(\n",
                "            self.parameters(), \n",
                "            lr=self.train_config.get('lr', 1e-4), \n",
                "            weight_decay=self.train_config.get('weight_decay', 0.05)\n",
                "        )\n",
                "        return optimizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "config = OmegaConf.create({\n",
                "    \"data\": {\n",
                "        \"train_data_dir\": \"/content/data/milk10k/train\",  # UPDATE THIS\n",
                "        \"val_data_dir\": \"/content/data/milk10k/val\",      # UPDATE THIS\n",
                "        \"img_size\": 224,\n",
                "        \"batch_size\": 32,\n",
                "        \"num_workers\": 2,\n",
                "    },\n",
                "    \"model\": {\n",
                "        \"num_classes\": 11,\n",
                "        \"in_chans\": 6,  # 6 Channels for stacked image\n",
                "        \"img_size\": 224,\n",
                "        \"patch_sizes\": [16, 8],\n",
                "        \"embed_dim\": 768,\n",
                "        \"depth\": 12,\n",
                "        \"num_heads\": 12\n",
                "    },\n",
                "    \"training\": {\n",
                "        \"epochs\": 30,\n",
                "        \"lr\": 1e-4,\n",
                "        \"weight_decay\": 1e-4,\n",
                "        \"precision\": \"16-mixed\",\n",
                "        \"focal_gamma\": 2.0\n",
                "    },\n",
                "    \"logging\": {\"log_every_n_steps\": 10},\n",
                "    \"paths\": { \"checkpoint_dir\": \"outputs/checkpoints\" }\n",
                "})\n",
                "\n",
                "# Set up data\n",
                "train_transform = get_train_transforms(config.model.img_size)\n",
                "val_transform = get_val_transforms(config.model.img_size)\n",
                "\n",
                "try:\n",
                "    # Check if data paths exist\n",
                "    train_dir = Path(config.data.train_data_dir)\n",
                "    if not train_dir.exists():\n",
                "        print(f\"Warning: {train_dir} not found. Please upload data.\")\n",
                "    else:\n",
                "        train_csv = Path(config.data.train_data_dir).parent / 'train.csv'\n",
                "        val_csv = Path(config.data.val_data_dir).parent / 'val.csv'\n",
                "        \n",
                "        train_dataset = MILK10kDataset(\n",
                "            root_dir=config.data.train_data_dir,\n",
                "            csv_file=str(train_csv),\n",
                "            transform=train_transform,\n",
                "            phase='train'\n",
                "        )\n",
                "        \n",
                "        val_dataset = MILK10kDataset(\n",
                "            root_dir=config.data.val_data_dir,\n",
                "            csv_file=str(val_csv),\n",
                "            transform=val_transform,\n",
                "            phase='val'\n",
                "        )\n",
                "        \n",
                "        dataloaders = create_dataloaders(train_dataset, val_dataset, batch_size=config.data.batch_size)\n",
                "        class_weights = torch.ones(config.model.num_classes)\n",
                "        \n",
                "        # Training\n",
                "        pl.seed_everything(42)\n",
                "        model = DermEquityModule(\n",
                "            model_config=OmegaConf.to_container(config.model),\n",
                "            train_config=OmegaConf.to_container(config.training),\n",
                "            class_weights=class_weights\n",
                "        )\n",
                "        \n",
                "        trainer = pl.Trainer(\n",
                "            max_epochs=config.training.epochs,\n",
                "            accelerator='auto',\n",
                "            devices=1,\n",
                "            precision=config.training.precision,\n",
                "            callbacks=[ModelCheckpoint(dirpath=config.paths.checkpoint_dir, monitor='val/auc_roc', mode='max')],\n",
                "            log_every_n_steps=config.logging.log_every_n_steps\n",
                "        )\n",
                "        \n",
                "        trainer.fit(model, dataloaders['train'], dataloaders['val'])\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Setup Error: {e}\")\n",
                "    print(\"Please verify your data paths in the config section!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}