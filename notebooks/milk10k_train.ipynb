{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MILK10k Skin Lesion Classification\n",
                "\n",
                "This notebook trains the **Tone-Aware Multi-Scale Vision Transformer (TAM-ViT)** on the MILK10k dataset.\n",
                "\n",
                "## ðŸš€ Setup\n",
                "\n",
                "1.  **Enable GPU**: Go to `Runtime` -> `Change runtime type` -> `T4 GPU` (or better).\n",
                "2.  **Upload Data**: You need to upload your `milk10k` dataset folder to your Drive or directly here.\n",
                "    - Expected structure:\n",
                "        ```\n",
                "        /content/data/milk10k/\n",
                "        â”œâ”€â”€ train/\n",
                "        â”‚   â”œâ”€â”€ lesion1_clin.jpg\n",
                "        â”‚   â”œâ”€â”€ lesion1_derm.jpg\n",
                "        â”‚   â””â”€â”€ ...\n",
                "        â”œâ”€â”€ train.csv\n",
                "        â””â”€â”€ val.csv\n",
                "        ```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install torch torchvision timm albumentations pandas numpy omegaconf pytorch-lightning wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Clone Repository (if running from Colab/Kaggle without local files)\n",
                "# If you uploaded the code manually, skip this.\n",
                "# !git clone https://github.com/your-username/derm-equity.git\n",
                "# %cd derm-equity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Import Libraries & Set Path\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to python path if needed\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "import torch\n",
                "from src.models.tam_vit import create_tam_vit_base\n",
                "from src.data.milk10k_dataset import MILK10kDataset\n",
                "from src.data.datasets import get_train_transforms, get_val_transforms, create_dataloaders\n",
                "from src.training.trainer import DermEquityModule, create_callbacks, create_loggers\n",
                "import pytorch_lightning as pl\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Configuration\n",
                "# We define the config here for easy editing in the notebook\n",
                "\n",
                "config = OmegaConf.create({\n",
                "    \"data\": {\n",
                "        \"dataset\": \"milk10k\",\n",
                "        \"train_data_dir\": \"/content/data/milk10k/train\",  # UPDATE THIS PATH\n",
                "        \"val_data_dir\": \"/content/data/milk10k/val\",      # UPDATE THIS PATH\n",
                "        \"img_size\": 224,\n",
                "        \"batch_size\": 32,\n",
                "        \"num_workers\": 2,\n",
                "        \"classes\": [\"AKIEC\", \"BCC\", \"BEN_OTH\", \"BKL\", \"DF\", \"INF\", \"MAL_OTH\", \"MEL\", \"NV\", \"SCCKA\", \"VASC\"]\n",
                "    },\n",
                "    \"model\": {\n",
                "        \"architecture\": \"tam_vit_base\",\n",
                "        \"num_classes\": 11,\n",
                "        \"in_chans\": 6,\n",
                "        \"pretrained\": True,\n",
                "        \"img_size\": 224\n",
                "    },\n",
                "    \"training\": {\n",
                "        \"epochs\": 30,\n",
                "        \"learning_rate\": 1e-4,\n",
                "        \"weight_decay\": 1e-4,\n",
                "        \"accumulate_grad_batches\": 1,\n",
                "        \"gradient_clip_val\": 1.0,\n",
                "        \"precision\": \"16-mixed\",\n",
                "        \"use_wandb\": False  # Set to True if you have a W&B account\n",
                "    },\n",
                "    \"optimizer\": {\"name\": \"adamw\", \"betas\": [0.9, 0.999]},\n",
                "    \"scheduler\": {\"name\": \"cosine_warmup\", \"warmup_epochs\": 3},\n",
                "    \"loss\": {\"name\": \"focal\", \"gamma\": 2.0},\n",
                "    \"logging\": {\"log_every_n_steps\": 10, \"wandb\": {\"project\": \"milk10k\", \"enabled\": False}},\n",
                "    \"paths\": {\n",
                "        \"output_dir\": \"outputs\",\n",
                "        \"checkpoint_dir\": \"outputs/checkpoints\",\n",
                "        \"log_dir\": \"outputs/logs\"\n",
                "    }\n",
                "})\n",
                "\n",
                "# Create directories\n",
                "Path(config.paths.output_dir).mkdir(parents=True, exist_ok=True)\n",
                "Path(config.paths.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
                "print(\"Configuration ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Load Data\n",
                "# Assumes train.csv and val.csv are in the parent directory of train_data_dir/val_data_dir\n",
                "# Adjust paths as needed for your upload structure\n",
                "\n",
                "train_transform = get_train_transforms(config.model.img_size)\n",
                "val_transform = get_val_transforms(config.model.img_size)\n",
                "\n",
                "try:\n",
                "    train_csv = Path(config.data.train_data_dir).parent / 'train.csv'\n",
                "    val_csv = Path(config.data.val_data_dir).parent / 'val.csv'\n",
                "    \n",
                "    train_dataset = MILK10kDataset(\n",
                "        root_dir=config.data.train_data_dir,\n",
                "        csv_file=str(train_csv),\n",
                "        transform=train_transform,\n",
                "        phase='train'\n",
                "    )\n",
                "    \n",
                "    val_dataset = MILK10kDataset(\n",
                "        root_dir=config.data.val_data_dir,\n",
                "        csv_file=str(val_csv),\n",
                "        transform=val_transform,\n",
                "        phase='val'\n",
                "    )\n",
                "    \n",
                "    print(f\"Train size: {len(train_dataset)}\")\n",
                "    print(f\"Val size: {len(val_dataset)}\")\n",
                "    \n",
                "    dataloaders = create_dataloaders(\n",
                "        train_dataset, val_dataset, \n",
                "        batch_size=config.data.batch_size, \n",
                "        num_workers=config.data.num_workers\n",
                "    )\n",
                "    \n",
                "    # Dummy class weights for now\n",
                "    class_weights = torch.ones(config.model.num_classes)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading data: {e}\")\n",
                "    print(\"Make sure your paths in Config are correct!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Training\n",
                "\n",
                "pl.seed_everything(42)\n",
                "\n",
                "# Initialize model\n",
                "model = DermEquityModule(\n",
                "    model_config=OmegaConf.to_container(config.model),\n",
                "    train_config=OmegaConf.to_container(config.training),\n",
                "    class_weights=class_weights\n",
                ")\n",
                "\n",
                "# Callbacks\n",
                "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
                "    dirpath=config.paths.checkpoint_dir,\n",
                "    monitor='val_auc',\n",
                "    mode='max',\n",
                "    filename='milk10k-{epoch:02d}-{val_auc:.4f}'\n",
                ")\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=config.training.epochs,\n",
                "    accelerator='auto',\n",
                "    devices=1,\n",
                "    precision=config.training.precision,\n",
                "    callbacks=[checkpoint_callback],\n",
                "    log_every_n_steps=config.logging.log_every_n_steps\n",
                ")\n",
                "\n",
                "# Start training\n",
                "trainer.fit(model, dataloaders['train'], dataloaders['val'])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}